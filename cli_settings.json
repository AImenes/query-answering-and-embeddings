{
    "seed": 41279374,
    "rewriting_upper_limit": 100,
    "n": 3,
    "k": 100,
    "tbox_import_file": "dbpedia15k.owl",
    ""
}


Define hyperparameters
RANDOM_SEED = 41279374  

#PerfectRef upper rewriting limit. Default = 100
rewriting_upper_limit = 100

#For Hits@N
n = 3

#Save top k predictions. This highly changes running time. Default = 100. 
k = 100 
#########################################################################
################          DEFAULT LOADING VALUES       ##################
tbox_import_file = "dbpedia15k.owl"
dataset = "dbpedia15k"
t_box_path = "dataset/"+dataset+"/tbox/"+tbox_import_file
a_box_path = "dataset/"+dataset+"/abox/transductive/"
tbox_ontology = load_ontology(t_box_path)
project_name = "001"
transductive_models = ["TransE", "BoxE", "RotatE", "DistMult", "CompGCN"]
tf = TriplesFactory.from_path(a_box_path + "all.txt")
train, valid, test = tf.split([0.8, 0.1, 0.1], random_state=RANDOM_SEED, method="cleanup")
current_model = None
current_model_params = {'selected_model_name': None, 'dim': None, 'epoch': None}
parsed_generated_queries = None
number_of_queries_per_structure = None
queries_from_generation = None
enable_online_lookup = True
base_cases_path  = "testcases/" + project_name + "/base_cases.pickle"

#Load previous base predictions
if os.path.exists(base_cases_path):
    with open(base_cases_path, 'rb') as file:
        base_cases = pickle.load(file)
else:
    base_cases = list()